regfit.full <- regsubsets(crim ~ ., data = boston.data, nvmax = 13)
summary(regfit.full)
# Best Subset Selection
regfit.full <- regsubsets(crim ~ ., data = boston.data, nvmax = 13)
reg.summary <- summary(regfit.full)
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
which.max(reg$summaryadjr2)
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
which.max(reg.summary$adjr2)
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
which.max(reg.summary$adjr2)
points(9, reg.summary$adjr2[9], col = "red", cex = 2, pch = 20)
coeff(regfit.full, 9)
coef(regfit.full, 9)
# Ridge Regression
x2 <- model.matrix(crim ~ ., boston.data)[, -1]
y2 <- boston.data$crim
ridge.boston <- glmnet(x, y, alpha = 0, lambda = lambda.grid)
summary(ridge.boston)
# Ridge Regression
x2 <- model.matrix(crim ~ ., boston.data)[, -1]
y2 <- boston.data$crim
set.seed(8)
cv.out <- cv.glmnet(x, y, alpha = 0)
bestlam3 <- cv.out$lambda.min
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam3)[1:13,]
# Ridge Regression
x2 <- model.matrix(crim ~ ., boston.data)[, -1]
y2 <- boston.data$crim
set.seed(8)
cv.out <- cv.glmnet(x2, y2, alpha = 0)
bestlam3 <- cv.out$lambda.min
out <- glmnet(x2, y2, alpha = 0)
predict(out, type = "coefficients", s = bestlam3)[1:13,]
# The Lasso
set.seed(9)
cv.out <- cv.glmnet(x2, y2, alpha = 1)
bestlam4 <- cv.out$lambda.min
out <- glmnet(x2, y2, alpha = 1, lambda = lambda.grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam4)[1:13,]
lasso.coef
# PCR
set.seed(11)
pcr.boston <- pcr(crim ~ ., data = boston.data, scale = T,
validation = "CV")
validationplot(pcr.boston, val.type = "MSEP")
# Fit PCR to entire data set using M = 8
pcr.boston <- pcr(y2 ~ x2, scale = T, ncomp = 5)
summary(pcr.boston)
# Use Validation-Set Approach to Determine Best Subset Selection Model
regfit.best <- regsubsets(crim ~ ., data = boston.data[train, ], nvmax = 13)
# Create test matrix
test.mat <- model.matrix(crim ~ ., data = boston.data[test, ])
# Compute test MSE for all possible amounts of variables used in the model
val.errors <- rep(NA, 13)
for (i in 1:13) {
coefi <- coef(regfit.best, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((boston.data$crim[test] - pred)^2)
}
# Get coefficient estimates for model with best subset of variables
best.subset <- which.min(val.errors)
coef(regfit.best, best.subset)
# Predict the number of applications using the PCR model
pcr.pred <- predict(pcr.fit, x[-train, ], ncomp = 17)
pcr.mse <- mean((pcr.pred - y[-train])^2)
pcr.mse
mse.models <- data.frame(
model = c("least.squares", "ridge.regression", "lasso", "pcr", "pls"),
mse = c(lm.mse, ridge.mse, lasso.mse, pcr.mse, pls.mse),
stringsAsFactors = F
)
mse.models
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
library(tree)
# Split data into training and test data
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
sales.test <- Carseats[-train, "Sales"]
tree.sales <- tree(Sales ~ ., data = Carseats, subset = train)
summary(tree.sales)
plot(tree.sales)
text(tree.sales, pretty = 0)
sales.pred <- predict(tree.sales, newdata = Carseats[-train, ])
plot(sales.pred, sales.test)
abline(0, 1)
mean((sales.pred - sales.test)^2)
cv.sales <- cv.tree(tree.sales)
plot(cv.sales$size, cv.sales$dev, type = "b")
prune.sales <- prune.tree(tree.sales, best = 14)
plot(prune.sales)
text(prune.sales, pretty = 0)
prune.pred <- predict(prune.sales, newdata = Carseats[-train, ])
plot(prune.pred, sales.test)
mean((prune.pred - sales.test)^2)
library(randomForest)
set.seed(2)
bag.sales <- randomForest(Sales ~ ., data = Carseats, subset = train,
mtry = (ncol(Carseats) - 1), importance = T)
bag.sales
bag.pred <- predict(bag.sales, newdata = Carseats[-train, ])
plot(bag.pred, sales.test)
abline(0, 1)
mean((bag.pred - sales.test)^2)
set.seed(2)
# Create random forest model using default m = p/3
rf.sales <- randomForest(Sales ~ ., data = Carseats, subset = train,
importance = T)
rf.pred <- predict(rf.sales, newdata = Carseats[-train, ])
mean((rf.pred - sales.test)^2)
set.seed(2)
# Create random forest model using default m = 6
rf.sales6 <- randomForest(Sales ~ ., data = Carseats, subset = train,
mtry = 6, importance = T)
rf.pred6 <- predict(rf.sales6, newdata = Carseats[-train, ])
mean((rf.pred6 - sales.test)^2)
set.seed(2)
# Create random forest model using default m = 9
rf.sales9 <- randomForest(Sales ~ ., data = Carseats, subset = train,
mtry = 9, importance = T)
rf.pred9 <- predict(rf.sales9, newdata = Carseats[-train, ])
mean((rf.pred9 - sales.test)^2)
importance(rf.sales6)
varImpPlot(rf.sales6)
# Analyze data using BART
library(BART)
x <- Carseats[, 2:11]
y <- Carseats[, "Sales"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
# Run BART with default settings
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
bart.pred <- bartfit$yhat.test.mean
mean((ytest - bart.pred)^2)
# Divide OJ data set into training and test data
set.seed(3)
train.oj <- sample(1:nrow(OJ), 800)
test.oj <- OJ[-train.oj, ]
# Fit a classification tree to OJ data with Purchase as the response
tree.oj <- tree(Purchase ~ ., data = OJ, subset = train.oj)
summary(tree.oj)
tree.oj
plot(tree.oj)
text(tree.oj, pretty = 0)
# Use classification tree to predict responses of test data
oj.pred <- predict(tree.oj, test.oj, type = "class")
# Generate confusion matrix
table(oj.pred, test.oj$Purchase)
(15 + 31) / (148 + 31 + 15 + 76)
# Use cross-validation to determine if pruning the tree may result in better prediction error
set.seed(4)
cv.oj <- cv.tree(tree.oj, FUN = prune.misclass)
names(cv.oj)
cv.oj
# Create plot of cross-validated training error over tree size
plot(cv.oj$size, cv.oj$dev, type = "b")
prune.oj <- prune.misclass(tree.oj, best = 5)
plot(prune.oj)
text(prune.oj, pretty = 0)
summary(prune.oj)
# Predict response with pruned tree
prune.oj.pred <- predict(prune.oj, test.oj, type = "class")
# Generate confusion matrix for pruned tree
table(prune.oj.pred, test.oj$Purchase)
(15 + 31) / (148 + 31 + 15 + 76)
?na.omit
hitters.data <- na.omit(Hitters)
View(hitters.data)
hitters.train <- c(1:200)
hitters.data <- na.omit(Hitters)
hitters.data$Salary <- log(hitters.data$Salary)
View(hitters.data)
hitters.train <- c(1:200)
hitters.test <- c(201:nrow(hitters.data))
install.packages("gbm")
library(gbm)
set.seed(5)
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4)
View(boost.hitters)
?rep
?gbm
library(gbm)
set.seed(5)
tunings <- c(0.001, 0.01, 0.2, 0.5, 1.0)
gbm.training <- data.frame(lambda = tunings,
training.error = rep(NA, 5))
for (x in length(tunings)) {
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = tunings[x])
gbm.training[x, "training.error"] <-  mean(boost.hitters$train.error)
}
gbm.training
library(gbm)
set.seed(5)
tunings <- c(0.001, 0.01, 0.2, 0.5, 1.0)
gbm.training <- data.frame(lambda = tunings,
training.error = rep(NA, 5))
for (x in 1:length(tunings)) {
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = tunings[x])
gbm.training[x, "training.error"] <-  mean(boost.hitters$train.error)
}
gbm.training
library(gbm)
set.seed(5)
tunings <- c(0.001, 0.01, 0.2, 0.5, 0.75, 1.0)
gbm.training <- data.frame(lambda = tunings,
training.error = rep(NA, length(tunings)))
for (x in 1:length(tunings)) {
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = tunings[x])
gbm.training[x, "training.error"] <-  mean(boost.hitters$train.error)
}
gbm.training
plot(gbm.training$lambda, gbm.training$training.error)
plot(gbm.training$lambda, gbm.training$training.error, type = "b")
gbm.test <- data.frame(lambda = tunings,
test.error = rep(NA, length(tunings)))
for (x in 1:length(tunings)) {
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = tunings[x])
yhat.boost <- predict(boost.hitters, newdata = hitters.data[hitters.test, ],
n.trees = 1000)
gbm.test[x, "test.error"] <-  mean((yhat.boost - hitters.data$Salary[hitters.test])^2)
}
gbm.test
plot(gbm.test$lambda, gbm.test$test.error, type = "b")
# Create LS regression with Salary as the predictor and determine test MSE
lm.hitters <- lm(Salary ~ ., data = hitters.data, subset = hitters.train)
lm.predict <- predict(lm.hitters, newdata = hitters.data[hitters.test, ])
lm.mse <- mean((lm.predict - hitters.data$Salary[hitters.test])^2)
lm.mse
?seq
x <- model.matrix(Salary ~ ., hitters.data)[-1]
y <- hitters.data$Salary
lambda.grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x[hitters.train, ], y[hitters.train], alpha = 1,
lambda = lambda.grid)
library(glmnet)
x <- model.matrix(Salary ~ ., hitters.data)[-1]
y <- hitters.data$Salary
lambda.grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x[hitters.train, ], y[hitters.train], alpha = 1,
lambda = lambda.grid)
?College
dim(x[hitters.train, ])
library(glmnet)
x <- model.matrix(Salary ~ ., hitters.data)[, -1]
y <- hitters.data$Salary
lambda.grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x[hitters.train, ], y[hitters.train], alpha = 1,
lambda = lambda.grid)
set.seed(7)
cv.out <- cv.glmnet(x[hitters.train, ], y[hitters.train], alpha = 1)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[hitters.test, ])
lasso.mse <- mean((lasso.pred - y[hitters.test])^2)
lasso.mse
# Create boosting model using shrinkage, or tuning parameter, equal to 0.01
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = 0.01)
summary(boost.hitters)
?Hitters
gbm.test <- data.frame(lambda = tunings,
test.error = rep(NA, length(tunings)))
for (x in 1:length(tunings)) {
set.seed(5)
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = tunings[x])
yhat.boost <- predict(boost.hitters, newdata = hitters.data[hitters.test, ],
n.trees = 1000)
gbm.test[x, "test.error"] <-  mean((yhat.boost - hitters.data$Salary[hitters.test])^2)
}
gbm.test
plot(gbm.test$lambda, gbm.test$test.error, type = "b")
set.seed(5)
bag.hitters <- randomForest(Salary ~ ., data = hitters.data,
subset = hitters.train, mtry = 19,
importance = T)
yhat.bag <- predict(bag.hitters, newdata = hitters.data[hitters.test, ])
mean((yhat.bag - hitters.data$Salary[hitters.test])^2)
knitr::opts_chunk$set(echo = TRUE)
# Create PLS model on the wine wine quality data
set.seed(2)
pls.fit <- plsr(quality ~ ., data = wine.data, subset = train.wine, scale = T,
validation = "CV")
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
library(tree)
library(randomForest)
library(gbm)
library(BART)
# Import wine quality data
wine.data <- read.csv("winequality-white.csv", sep=";", na.strings = "?", stringsAsFactors = T)
View(wine.data)
# Create training and test data
set.seed(10)
train.wine <- sample(1:nrow(wine.data), 0.5 * nrow(wine.data))
test.wine <- (-train.wine)
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(quality ~ ., wine.data)[, -1]
y <- wine.data$quality
y.test <- y[test.wine]
# Create PLS model on the wine wine quality data
set.seed(2)
pls.fit <- plsr(quality ~ ., data = wine.data, subset = train.wine, scale = T,
validation = "CV")
summary(pls.fit)
# Plot MSEP over the number of components
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
library(reticulate)
library(keras)
library(tensorflow)
use_virtualenv("my_tf_workspace", required = TRUE)
?unlist
mtcars <- mtcars
mtcars_x <- mtcars[, c("cyl", "disp", "hp")]
mtcars_x <- array(data = unlist(mtcars_x),
dim = c(32, 3),
dimnames = list(rownames(mtcars_x),
colnames(mtcars_x)))
mtcars_y <- mtcars[, "mpg"]
nn_model <- keras_model_sequential() %>%
layer_dense(units = 1, input_shape = 3, activation = "linear")
nn_model
nn_model <- keras_model_sequential() %>%
layer_dense(units = 2, input_shape = 3, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
nn_model
nn_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.2),
loss = "mean_squared_error",
metrics = "mean_absolute_error")
nn_model_training <- nn_model %>% fit(x = mtcars_x,
y = mtcars_y,
epochs = 250,
verbose = FALSE)
plot(nn_model_training)
get_weights(nn_model)
?knitr::opts_chunk
install.packages("caret")
setwd("C:/Users/janna/Documents/Merrimack MSDS/DSE6211/Week 3")
library(dplyr)
library(caret)
data <- read.csv("lab_3_data/lab_3_data.csv")
View(data)
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = False,
times = 1)
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = F,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
unique(training_set$wilderness_area)
unique(training_set$soil_type)
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n())
View(top_20_soil_types)
?n()
?select
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
View(top_20_soil_types)
?ifelse
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
class(training_set$wilderness_area)
class(training_set$soil_type)
levels(training_set$wilderness_area)
levels(training_set$soil_type)
View(training_set)
data <- read.csv("lab_3_data/lab_3_data.csv")
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
levels(training_set$soil_type)
View(training_set)
library(dplyr)
library(caret)
data <- read.csv("lab_3_data/lab_3_data.csv")
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = F,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
unique(training_set$wilderness_area)
unique(training_set$soil_type)
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
class(training_set$wilderness_area)
class(training_set$soil_type)
levels(training_set$wilderness_area)
levels(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area", "soil_type")],
levelsOnly = T,
fullRank = T)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area", "soil_type")])
View(onehot_enc_training)
training_set <- cbind(training_set, onehot_enc_training)
library(dplyr)
library(caret)
data <- read.csv("lab_3_data/lab_3_data.csv")
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = F,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
unique(training_set$wilderness_area)
unique(training_set$soil_type)
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
class(training_set$wilderness_area)
class(training_set$soil_type)
levels(training_set$wilderness_area)
levels(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area", "soil_type")],
levelsOnly = T,
fullRank = T)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[, -c(11:13)], 2, mean),
scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[, -c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(training_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(test_set)))
head(training_features)
head(test_features)
dim(training_features)
View(top_20_soil_types)
View(test_set)
View(onehot_enc_test)
View(onehot_enc_test)
